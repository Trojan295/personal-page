[{"content":"For a personal project, I was experimenting with DDD. I was sending domain events through RabbitMQ to run choreography-based sagas. One problem I had was ensuring that the domain event got sent out after modifying aggregates. It\u0026rsquo;s not possible to run an atomic transaction through MongoDB and RabbitMQ, so there can be a situation where the aggregate is modified successfully in the database, but we won\u0026rsquo;t send an event because RabbitMQ is not available. You could use retries, but this won\u0026rsquo;t withstand, for example, an application crash.\nTo solve this, I used the transactional outbox pattern. The idea is to have an additional table in the database to save events that have to be sent. It must be in the same database where the aggregate is persisted, so you can run it in the same DB transaction. Then we have some other process that polls documents from this collection and sends them to the event bus or queue.\nWith this, we can modify the aggregate and dispatch the event in the same MongoDB transaction. The event will then be sent to the RabbitMQ when it\u0026rsquo;s available. This pattern ensures that the event will be sent out at least once, but it could also be sent multiple times. So on the consumer side, you have to either be idempotent or check for duplicate events. One idea here is to add GUIDs to your events and use the inbox pattern, where you check for duplicate events.\nGolang implementation I was able to find a lot of examples in C# and Java, but just a few for the Outbox pattern in Golang. My example here is not something you can copy-paste, but you should get an idea of how to implement it yourself.\nI defined the following interfaces:\npackage application // EventPublisher publishes events to an event bus or queue. type EventPublisher interface { PublishEvents(ctx context.Context, event ...*domain.Event) error } // EventOutbox dispatches events to the transactional outbox. type EventOutbox interface { DispatchEvents(ctx context.Context, event ...*domain.Event) error } // UnitOfWork provides an interface for running operations on the persistance layer in a single transaction. type UnitOfWork interface { OrderRepository() domain.OrderRepository EventOutbox() domain.EventOutbox Run(ctx context.Context, f func(ctx context.Context) (interface{}, error)) (interface{}, error) } I have all the interactions with MongoDB in a single Go struct. My MongoDBStore implements the EventOutbox and UnitOfWork interfaces. It also has a method RunOutbox to run the process, which sends events from the outbox to the event bus.\n// Event is MongoDB event representation type Event struct { ID primitive.ObjectID `bson:\u0026#34;_id,omitempty\u0026#34;` Data bson.Raw `bson:\u0026#34;data\u0026#34;` Published bool `bson:\u0026#34;published\u0026#34;` } // ToModel is used convert from MongoDB to domain event func (dto *Event) ToModel() (*domain.Event, error) { // ... } // FromModel is used to convert domain event to MongoDB representation func (dto *Event) FromModel(event *domain.Event) error { // ... } // Run runs f in a single MongoDB transaction. func (s *MongoDBStore) Run(ctx context.Context, f func(ctx context.Context) (interface{}, error)) (interface{}, error) { session, err := s.client.StartSession() if err != nil { return nil, err } defer session.EndSession(ctx) result, err := session.WithTransaction(ctx, func(sessCtx mongo.SessionContext) (interface{}, error) { return f(sessCtx) }) if err != nil { return nil, err } return result, nil } // DispatchEvent inserts an event in the outbox collection to send out. func (store *MongoDBStore) DispatchEvent(ctx context.Context, event *domain.Event) error { collection := store.client.Database(store.database).Collection(outboxCollection) dto := \u0026amp;Event{} if err := dto.FromModel(event); err != nil { return fmt.Errorf(\u0026#34;failed to convert from model: %w\u0026#34;, err) } _, err := collection.InsertOne(ctx, dto) if err != nil { return fmt.Errorf(\u0026#34;failed to insert event: %w\u0026#34;, err) } return nil } // RunOutbox runs an infinite loop, which polls and sends events func (store *MongoDBStore) RunOutbox(ctx context.Context, eventPublisher application.EventPublisher) error { ticker := time.NewTicker(1 * time.Second) for { select { case \u0026lt;-ctx.Done(): return case \u0026lt;-ticker.C: if err := store.runOutbox(ctx, eventPublisher); err != nil { return err } } } } func (store *MongoDBStore) runOutbox(ctx context.Context, eventPublisher application.EventPublisher) error { events, err := store.getUnpublishedEvents(ctx) if err != nil { return fmt.Errorf(\u0026#34;failed to get unpublished events: %w\u0026#34;, err) } for _, dto := range events { event, err := dto.ToModel() if err != nil { return fmt.Errorf(\u0026#34;failed to convert event to model: %w\u0026#34;, err) } if err := eventPublisher.PublishEvents(ctx, event); err != nil { return fmt.Errorf(\u0026#34;failed to publish event: %w\u0026#34;, err) } if err := store.setEventAsPublished(ctx, dto.ID); err != nil { return fmt.Errorf(\u0026#34;failed to set event as published: %w\u0026#34;, err) } } return nil } func (store *MongoDBStore) getUnpublishedEvents(ctx context.Context) ([]Event, error) { collection := store.client.Database(store.database).Collection(outboxCollection) cursor, err := collection.Find(ctx, bson.M{\u0026#34;published\u0026#34;: false}) if err != nil { return nil, fmt.Errorf(\u0026#34;failed to get unpublished events: %w\u0026#34;, err) } var events []Event if err := cursor.All(ctx, \u0026amp;events); err != nil { return nil, fmt.Errorf(\u0026#34;failed to decode unpublished events: %w\u0026#34;, err) } return events, nil } func (store *MongoDBStore) setEventAsPublished(ctx context.Context, eventID primitive.ObjectID) error { collection := store.client.Database(store.database).Collection(outboxCollection) result := collection.FindOneAndUpdate(ctx, bson.M{\u0026#34;_id\u0026#34;: eventID}, bson.M{\u0026#34;$set\u0026#34;: bson.M{\u0026#34;published\u0026#34;: true}}) if result.Err() != nil { return fmt.Errorf(\u0026#34;failed to set event as published: %w\u0026#34;, result.Err()) } return nil } In main, you have to run the RunOutbox method in a goroutine. You can then use this in your handlers to modify aggregates and send domain events:\npackage application type CreateOrderHandler struct { uow UnitOfWork } func (h CreateOrderHandler) Handle(ctx context.Context, cmd CreateOrder) (*domain.Order, error) { // this generates an OrderCreated event order := domain.CreateOrder() orderIface, err := h.uow.Run(ctx, func(ctx context.Context) (interface{}, error) { if err := h.uow.OrderRepository().CreateOrder(ctx, order); err != nil { return nil, fmt.Errorf(\u0026#34;failed to create order: %w\u0026#34;, err) } if err := h.uow.EventOutbox().DispatchEvents(order.Events()); err != nil { return nil, fmt.Errorf(\u0026#34;failed to dispatch events: %w\u0026#34;, err) } return order, nil }) if err != nil { return nil, err } return orderIface.(*domain.Order), nil } You could also use MongoDB ChangeStreams instead of polling to get information when a new event is in the outbox.\n","permalink":"https://myhightech.org/posts/20230702-transactional-inbox-pattern/","summary":"For a personal project, I was experimenting with DDD. I was sending domain events through RabbitMQ to run choreography-based sagas. One problem I had was ensuring that the domain event got sent out after modifying aggregates. It\u0026rsquo;s not possible to run an atomic transaction through MongoDB and RabbitMQ, so there can be a situation where the aggregate is modified successfully in the database, but we won\u0026rsquo;t send an event because RabbitMQ is not available.","title":"Transactional outbox pattern example in Golang and MongoDB"},{"content":"I decided to make a habit of learning at least one new tool, technology every month and prepare a blog post about it. In recent months I focused mostly on the stuff I was dealing at work. Right now I\u0026rsquo;m reading \u0026ldquo;Atomic Habits\u0026rdquo; by James Clear, so that\u0026rsquo;s the first habit I would like to create. :)\nWhat is Pulumi This month I decided to check out Pulumi. Pulumi is an open-source infrastructure as code tool. It tackles the same problem Terraform, CloudFormation or ARM templates do, so to define, create and manage cloud infrastructure via source code. The thing that sets Pulumi apart is the fact, that you use a general-purpose programming language to define infrastructure. Languages supported currently are TypeScript, JavaScript, Python and Go. This makes Pulumi an interesting choice for software developers, who have to manage infrastructure, but don\u0026rsquo;t want to learn a new language.\nI tried to deploy a few common infrastructure parts, so I can compare Pulumi to Terraform and CloudFormation.\nCases Deploying a simple AWS VPC To start off, I always need a VPC on AWS. So I need to create:\nVPC Private and public subnets per availability zone Internet Gateway NAT Gateway In Pulumi I have to start by creating a project (could be compared to a Terraform root module or CloudFormation template). This is where you write your program and where you can define stacks (could be compared to Terraform workspaces and CloudFormation stacks). Pulumi, similarly to Terraform, also stores the infrastructure metadata in a state. Unlike Terraform, they encourage you to use their free hosted backend to store the state, so you don\u0026rsquo;t have to provision an S3 bucket and do the state management on your own. Cool, I don\u0026rsquo;t really like the state management in Terraform (You need a S3 bucket, but how to create it? A chicken or egg problem). But you store and manage the state on your own S3 bucket or other backend, if you want. You can read more about the Pulumi concepts here.\nI decided to write the infrastructure code in Go. My code for the simple VPC is available here. To be honest, I have mixed feelings about it. It feels verbose, and I had to write quite a lot of code to achieve a really basic VPC setup. A lot of code is about transforming values to the correct Go structs for Pulumi. AFAIK it\u0026rsquo;s required so Pulumi can track the dependencies and build a DAG for the resources.\n// Create a AWS VPC resource vpc, err := ec2.NewVpc(ctx, \u0026#34;vpc\u0026#34;, \u0026amp;ec2.VpcArgs{ CidrBlock: pulumi.String(\u0026#34;10.0.0.0/16\u0026#34;), Tags: pulumi.ToStringMap(map[string]string{ \u0026#34;Name\u0026#34;: \u0026#34;my-vpc\u0026#34;, }), }) I don\u0026rsquo;t really like the amount of code I had to write. Someone could argue that in CloudFormation or Terraform you also have to write a lot of code. Right, but HCL or YAML feels to me much lighter and easier to use. I don\u0026rsquo;t have to deal with the complexity of a general-purpose language.\nHaving in mind, that it\u0026rsquo;s a general-purpose language I tried to search for libraries, which could lower the amount of code I have to write. In Terraform I very often use modules like this VPC module, so can have a full-blown VPC using a dozen lines. To my surprise, I was not able to find anything in Go! I found Pulumi Crosswalk, but it\u0026rsquo;s only for TypeScript.\nI decided to try to modularize the code on my own.\nCreating an AWS VPC Pulumi module As I was not able to find any library, which would simplify my VPC code, I prepared a Go package for a complete VPC module. The code is here. It creates a VPC, subnets, Internet gateway and NAT gateway and can be used the following way:\nimport ( \u0026#34;github.com/Trojan295/pulumi-poc/pkg/vpc\u0026#34; ) func main() { pulumi.Run(func(ctx *pulumi.Context) error { vpcOutput, err := vpc.NewVpc(ctx, \u0026amp;vpc.VpcInput{ VpcCidrBlock: \u0026#34;10.0.0.0/16\u0026#34;, AvailabilityZones: []string{\u0026#34;eu-west-1a\u0026#34;, \u0026#34;eu-west-1b\u0026#34;, \u0026#34;eu-west-1c\u0026#34;}, PrivateSubnetCidrBlocks: []string{\u0026#34;10.0.0.0/24\u0026#34;,\u0026#34;10.0.1.0/24\u0026#34;,\u0026#34;10.0.2.0/24\u0026#34;}, PublicSubnetCidrBlocks: []string{\u0026#34;10.0.100.0/24\u0026#34;,\u0026#34;10.0.101.0/24\u0026#34;,\u0026#34;10.0.102.0/24\u0026#34;}, }) if err != nil { return err } return nil }) } That looks a lot better to me and is kinda similar to how modules are used in Terraform. I updated my VPC project to use my VPC module. It\u0026rsquo;s available here.\nHost an application in the VPC Having a VPC I can move to the next part: deploying some application. I took a really basic example: EC2 instances in an Autoscaling Group behind an Elastic LoadBalancer. I wanted to use this exercise to try out another feature of Pulumi: Stack references. Such pattern is very common in Terraform and CloudFormation. You keep a VPC in one Terraform state and then use a remote state data source to use it in another. It makes it easier to manage large cloud deployments.\nFollowing the module approach I took with the VPC, I prepared some small modules for the SecurityGroup, ELB and ASG and created another Pulumi project, which uses those modules and references the VPC project. Code for the application Pulumi project is here.\nIt doesn\u0026rsquo;t look bad, although I don\u0026rsquo;t like the transformations I need to do to read the subnet IDs from the VPC stack.\nvpcStackName := fmt.Sprintf(\u0026#34;Trojan295/vpc-modular/%s\u0026#34;, ctx.Stack()) vpcStack, _ := pulumi.NewStackReference(ctx, vpcStackName, nil) publicSubnets := vpcStack.GetOutput(pulumi.String(\u0026#34;publicSubnetIDs\u0026#34;)).ApplyT(func(x interface{}) []string { y := x.([]interface{}) r := make([]string, 0) for _, item := range y { r = append(r, item.(string)) } return r }).(pulumi.StringArrayOutput) Again the requirement to transform the parameters to the Pulumi input and output structs requires some boilerplate code. I have not tried to use another programming language, maybe in JavaScript or Python it looks simpler.\nMy overall experience I tried to deploy some simple infrastructure parts using Pulumi to get a grasp on how it feels. I have mixed feelings about it.\nPulumi has the gimmick of using a general-purpose programming language, but to be honest I expected more. The ecosystem around isn\u0026rsquo;t as mature as Terraform or CloudFormation. I though a big advantage would be, that you can leverage the packaging of the selected language and use code writing by other people, but in case of Go I couldn\u0026rsquo;t find any libraries, which would implement good practice infrastructure blocks.\nHCL or YAML are simpler than Go. A Terraform module is a flat project, where you define the resources and modules you want to get and there isn\u0026rsquo;t much complexity behind it. When using a language like Go or JavaScript you will need to deal with loops, functions, classes, error handling etc. and I don\u0026rsquo;t see much advantage in it.\nMaybe there are some special cases, where Pulumi can shine. I.e. when you have some custom logic, which you have to execute during the deployment, and it\u0026rsquo;s hard to integrate when using other IaC tools. In this case using a general-purpose language could make sense. Right now, I think for most cases dedicated IaC tools like Terraform, CloudFormation or ARM templates are enough. They are mature and have established communities.\nRead more https://www.pulumi.com/ https://www.pulumi.com/docs/get-started/ https://www.pulumi.com/docs/guides/crosswalk/aws/ https://github.com/pulumi/examples ","permalink":"https://myhightech.org/posts/20210915-pulumi-check/","summary":"I decided to make a habit of learning at least one new tool, technology every month and prepare a blog post about it. In recent months I focused mostly on the stuff I was dealing at work. Right now I\u0026rsquo;m reading \u0026ldquo;Atomic Habits\u0026rdquo; by James Clear, so that\u0026rsquo;s the first habit I would like to create. :)\nWhat is Pulumi This month I decided to check out Pulumi. Pulumi is an open-source infrastructure as code tool.","title":"Learning stuff: Pulumi for AWS"},{"content":" I bought recently two Odroid HC4 microcomputers and 4 Seagate Barracuda 2 TB HDD with the intention to farm Chia on them. The HC4 platform used on ARM64 processor and has two SATA 3, which can be used to plug HDD or SSD drive to it. At work, we are checking out currently the OpenEBS project, which allows to build a Container Attached Storage for Kubernetes using local disks. I was curious, if I could use OpenEBS on those HC4 devices and create a CAS for a Kubernetes home lab.\nHardware At Botland I bought the Odroid HC4 devices and the required accessories:\n2 x Odroid HC4 - Amlogic S905X3 Quad-Core 1,8GHz + 4GB RAM — 838 zł 2 x SanDisk Ultra 64 GB SD card, class 10 — 87,80 zł 2 x 15V/4A power supply — 125 zł At xkom I bought four HDDs:\n4 x Seagate Barracuda 2TB, 7200 RPM (ST2000DM008) — 956 zł So the final bill was 2006,80 zł, which is around $550.\nConfiguration Installing k3s on Odroid HC4 I flashed the official Ubuntu Minimal images from hardkernel on the SD cards, plugged the hard drives to the SATA 3 slots and powered on the HC4 computers. First thing I noticed was, that the hard drive are loud. Very loud. I was using only SSDs on my PC for a couple of years and I forgot the sound of spinning disks. I wouldn\u0026rsquo;t put this in my bedroom.\nI configured static IPs for the Odroids on my home router and decided to use k3s to create a Kubernetes cluster. I also changed the hostnames of the Odroids:\nHostname IP address Role odroid-hc4-01 192.168.1.210 K3s server odroid-hc4-02 192.168.1.211 K3s agent To create the k3s cluster I used k3sup:\n# Install k3s server node $ k3sup install --ip 192.168.1.210 --user root # Join k3s agent node $ k3sup join --ip 192.168.1.211 --server-ip 192.168.1.210 --user root k3sup saves the kubeconfig file to the cluster in your working directory, so you can configure kubectl to the k3s cluster by executing:\n$ export KUBECONFIG=$PWD/kubeconfig $ kubectl get nodes NAME STATUS ROLES AGE VERSION odroid-hc4-01 Ready master 41h v1.19.11+k3s1 odroid-hc4-02 Ready \u0026lt;none\u0026gt; 41h v1.19.11+k3s1 After I had the Kubernetes cluster ready I moved on to install OpenEBS on it.\nInstalling OpenEBS Using OpenEBS you can create a Container Attached Storage using local disks on your cluster nodes. This is useful, when you aren\u0026rsquo;t running on a cloud provider with a managed block storage offering or when you want to create a replicated storage class for a Kubernetes cluster in your home lab, and you don\u0026rsquo;t have an iSCSI disk array.\nI followed the installation guide on their official documentation:\n# Install and enable iSCSI initiator $ apt-get install open-iscsi $ systemctl enable --now iscsid # Install OpenEBS using Helm $ helm repo add openebs https://openebs.github.io/charts $ helm repo update $ helm install --namespace openebs --create-namespace openebs openebs/openebs The OpenEBS Node Manager should automatically find available, unmounted disks on the nodes. I verified, if OpenEBS discovered my HDD drives and installed to Storage Classes:\n$ kubectl get blockdevice -n openebs NAME NODENAME SIZE CLAIMSTATE STATUS AGE blockdevice-1a0d6f15e0044f96487d1529a4446e9a odroid-hc4-01 2000397868544 Unclaimed Active 24h blockdevice-5b01691835dd756c9fcdcfe6bd953b63 odroid-hc4-01 2000397868544 Unclaimed Active 24h blockdevice-f9537c7636fc1692ac5cf7dbc1209122 odroid-hc4-02 2000397868544 Unclaimed Active 20h blockdevice-7961db657eb3e5da183d5a281ffb1599 odroid-hc4-02 2000397868544 Unclaimed Active 24h $ kubectl get storageclasses NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE local-path (default) rancher.io/local-path Delete WaitForFirstConsumer false 41h openebs-snapshot-promoter volumesnapshot.external-storage.k8s.io/snapshot-promoter Delete Immediate false 41h openebs-hostpath openebs.io/local Delete WaitForFirstConsumer false 41h openebs-device openebs.io/local Delete WaitForFirstConsumer false 41h openebs-jiva-default openebs.io/provisioner-iscsi Delete Immediate false 41h So now we have 3 additional storage classes to use:\nopenebs-hostpath (Local PV Hostpath) — creates a PV using local hostpath, by default /var/openebs/local. So you could mount for e.g. your disk under this path, and it will be used for the PVs. openebs-device (Local PV Device) — claims an available block device, mounts it, creates a filesystem and uses for the PV. Using this class you get the full performance of the drive, but you can create only 1 PV per blockdevice, and it is not replicated. openebs-jiva-default (Jiva) — uses the Jiva storage engine to provision PVs. It\u0026rsquo;s an older solution and is only recommended for smaller workload and when you don\u0026rsquo;t need features like snapshots. There are also two other storage engines, which you have to install separably:\ncStor - most mature and recommended engine. Supports SAN multipath and snapshots, so can be considered for production and HA workloads. Drawback is, that it\u0026rsquo;s really slow, compared to the raw device performance. Mayastor - new, still experimental storage engine. Offers much better performance than Jiva and cStor, but for now lacks some features like multipath or snapshots. It also does not support ARM64 architecture so I wasn\u0026rsquo;t able to test Mayastor on my Odroid cluster. Installing cStor The cStor operator and provisioner must be installed separately. I used their Helm chart to install it:\n$ helm repo add openebs-cstor https://openebs.github.io/cstor-operators $ helm install openebs-cstor openebs-cstor/cstor -n openebs --set openebsNDM.enabled=false To use cStor you have to create an cStorPoolCluster. A cStor pool are one or more nodes on a single node, which are designated to create PVs. A pool cluster are multiple pools from different nodes. This allows for PV replication and provides durability and HA.\nI created the following cStorPoolCluster CR:\n# cspc.yaml apiVersion: cstor.openebs.io/v1 kind: CStorPoolCluster metadata: name: cstor-storage namespace: openebs spec: pools: - nodeSelector: kubernetes.io/hostname: \u0026#34;odroid-hc4-01\u0026#34; dataRaidGroups: - blockDevices: - blockDeviceName: \u0026#34;blockdevice-1a0d6f15e0044f96487d1529a4446e9a\u0026#34; - blockDeviceName: \u0026#34;blockdevice-5b01691835dd756c9fcdcfe6bd953b63\u0026#34; poolConfig: dataRaidGroupType: \u0026#34;stripe\u0026#34; - nodeSelector: kubernetes.io/hostname: \u0026#34;odroid-hc4-02\u0026#34; dataRaidGroups: - blockDevices: - blockDeviceName: \u0026#34;blockdevice-f9537c7636fc1692ac5cf7dbc1209122\u0026#34; - blockDeviceName: \u0026#34;blockdevice-7961db657eb3e5da183d5a281ffb1599\u0026#34; poolConfig: dataRaidGroupType: \u0026#34;stripe\u0026#34; dataRaidGroupType: stripe means that the capacity of the pool will be the sum of the 2 block devices. So in my case the capacity of the whole pool cluster is 8 TB. You could also set dataRaidGroupType: mirror to get more resilience in case of a disk failure, at the cost of capacity.\nThe last thing is to create the Storage Class for cStor. We have to provide the pool cluster for the class and the desired replica count. Note, that the replica count must be equal or lower to the number of pools in the pool cluster. Again, depending on your use case, you can balance between resilience and capacity. I choose to maximize the capacity of my pool cluster and set the replica count to 1:\n# cstor-csi.yaml kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: cstor-csi provisioner: cstor.csi.openebs.io allowVolumeExpansion: true parameters: cas-type: cstor cstorPoolCluster: cstor-storage replicaCount: \u0026#34;1\u0026#34; $ kubectl apply -f cspc.yaml $ kubectl apply -f cstor-csi.yaml $ k get sc cstor-csi NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE cstor-csi cstor.csi.openebs.io Delete Immediate true 7s Benchmarking the Storage Classes Setup I was curious what is the performance of the different Storage Classes. I\u0026rsquo;m using only HDD drives, so I wasn\u0026rsquo;t expecting high numbers, especially on random read/write.\nFor benchmarking, I used kubestr. It\u0026rsquo;s a simple tool, which creates a Pod with a PVC from the selected Storage Class and runs fio in the pod to benchmark it. Unfortunately, they didn\u0026rsquo;t have a container image for ARM64 architecture, so I forked the repository and built and ARM64 compatible image:\nGitHub Action — https://github.com/Trojan295/kubestr/actions/runs/886652128 Docker image — ghcr.io/trojan295/kubestr:add-arm64-docker-image I also prepared bunch of fio configs based on this page, so I could benchmark random RW and sequential read throughput and IOPS. I used the following configs:\nRandom read/write IOPS [randrw_iops] bs=4K iodepth=256 name=raw-randreadwrite rw=randrw size=2G ioengine=libaio direct=1 group_reporting time_based runtime=120 numjobs=4 Random read/write throughput [randrw_iops] bs=64K iodepth=64 name=raw-randreadwrite rw=randrw size=2G ioengine=libaio direct=1 group_reporting time_based runtime=120 numjobs=4 Sequential read IOPS [seqread_iops] bs=4K iodepth=256 name=raw-read rw=read size=2G ioengine=libaio direct=1 group_reporting time_based runtime=120 numjobs=4 Sequential read throughput [seqread_speed] bs=64K iodepth=64 name=raw-read rw=read size=2G ioengine=libaio direct=1 group_reporting time_based runtime=120 numjobs=4 I benchmarked the following storage classes:\nopenebs-device openebs-jiva-default cstor-csi I was running kubestr with the following command:\nkubestr fio --image ghcr.io/trojan295/kubestr -s \u0026lt;storage-class\u0026gt; --fiofile \u0026lt;config-file\u0026gt; Results The Local PV device results are very similar to the results on UserBenchmark for this HDD. Sequential read performance is killed by using either Jiva or cStor. For random read/write there is a 40% throughput drop. The IOPS on the cStor Storage Class is higher than on the Local PV device most probably because in have 2 disks in stripe mode in my cStor pool.\nMy thoughts It took my a single day to create and configure this setup and it wasn\u0026rsquo;t hard. Reading the OpenEBS documentation is enough to get started. I haven\u0026rsquo;t tried to tune the performance of Jiva and cStor pools, so most probably I could get more performance out of those disks after playing with some parameters.\nI think OpenEBS is a nice solution, when you are running Kubernetes in your home lab, and you need some CAS or when you have an on-premise setup. When using managed solutions like AWS EKS or Azure AKS I think it\u0026rsquo;s better to use their block storage offering. You could use OpenEBS to create an HA storage solution over few AWS availability zones (EBS are tight to a single AZ), but you need to remember, that the volume replication is done synchronous, so the performance will be worse.\nFrom what I saw the experimental Mayastor provides much better performance (some benchmarks show it\u0026rsquo;s comparable to the raw device performance). Unfortunately, it does not support ARM64 for now, although there is some work going on in this field: https://github.com/openebs/Mayastor/pull/671.\n","permalink":"https://myhightech.org/posts/20210530-openebs-on-odroid-hc4/","summary":"I bought recently two Odroid HC4 microcomputers and 4 Seagate Barracuda 2 TB HDD with the intention to farm Chia on them. The HC4 platform used on ARM64 processor and has two SATA 3, which can be used to plug HDD or SSD drive to it. At work, we are checking out currently the OpenEBS project, which allows to build a Container Attached Storage for Kubernetes using local disks. I was curious, if I could use OpenEBS on those HC4 devices and create a CAS for a Kubernetes home lab.","title":"Running OpenEBS on a Odroid HC4 cluster"},{"content":"Integrating Cert Manager with Route53 on EKS In this article I will show, how you can automatically get Let\u0026rsquo;s Encrypt SSL certificates using Cert Manager. We will leverage the DNS01 challenge and use a Route53 Hosted Zone to answer the challenge. The Cert Manager will use an EKS IAM Role Service Account, which follows AWS best practices for security.\nSet up the EKS cluster with Terraform We will use this EKS module to provision our EKS cluster. You have to set the enable_irsa parameter to true. This registers the EKS clusters OpenID Connect server as a provider for the AWS IAM, which allows Kubernetes service accounts to assume IAM roles on our AWS account.\nWe also create an IAM role for the Cert Manager service account, which has permissions to the Route53 Hosted Zone.\nCreate a Terraform module with the following content:\n# Variables variable \u0026#34;name\u0026#34; { type = string } variable \u0026#34;domain_name\u0026#34; { type = string } variable \u0026#34;region\u0026#34; { type = string default = \u0026#34;eu-west-1\u0026#34; } # Providers provider \u0026#34;aws\u0026#34; { region = var.region } provider \u0026#34;kubernetes\u0026#34; { host = data.aws_eks_cluster.cluster.endpoint cluster_ca_certificate = base64decode(data.aws_eks_cluster.cluster.certificate_authority.0.data) token = data.aws_eks_cluster_auth.cluster.token } data \u0026#34;aws_eks_cluster\u0026#34; \u0026#34;cluster\u0026#34; { name = module.eks.cluster_id } data \u0026#34;aws_eks_cluster_auth\u0026#34; \u0026#34;cluster\u0026#34; { name = module.eks.cluster_id } # VPC locals { eks_cluster_name = \u0026#34;${var.name}-eks\u0026#34; route53_zone_id = module.zones.this_route53_zone_zone_id[var.domain_name] } module \u0026#34;vpc\u0026#34; { source = \u0026#34;terraform-aws-modules/vpc/aws\u0026#34; name = \u0026#34;${var.name}-vpc\u0026#34; cidr = \u0026#34;10.0.0.0/16\u0026#34; azs = [\u0026#34;eu-west-1a\u0026#34;, \u0026#34;eu-west-1b\u0026#34;, \u0026#34;eu-west-1c\u0026#34;] private_subnets = [\u0026#34;10.0.0.0/23\u0026#34;, \u0026#34;10.0.2.0/23\u0026#34;, \u0026#34;10.0.4.0/23\u0026#34;] public_subnets = [\u0026#34;10.0.100.0/23\u0026#34;, \u0026#34;10.0.102.0/23\u0026#34;, \u0026#34;10.0.104.0/23\u0026#34;] enable_dns_hostnames = true enable_nat_gateway = true single_nat_gateway = true private_subnet_tags = { \u0026#34;kubernetes.io/cluster/${local.eks_cluster_name}\u0026#34; : \u0026#34;shared\u0026#34; } public_subnet_tags = { \u0026#34;kubernetes.io/cluster/${var.name}-eks\u0026#34; : \u0026#34;shared\u0026#34; } } # EKS module \u0026#34;eks\u0026#34; { source = \u0026#34;terraform-aws-modules/eks/aws\u0026#34; cluster_name = local.eks_cluster_name cluster_version = \u0026#34;1.18\u0026#34; vpc_id = module.vpc.vpc_id subnets = concat(module.vpc.private_subnets, module.vpc.public_subnets) manage_aws_auth = true enable_irsa = true worker_groups = [ { instance_type = \u0026#34;t3a.medium\u0026#34; asg_max_size = 2 asg_desired_capacity = 2 root_volume_type = \u0026#34;gp2\u0026#34; subnets = module.vpc.private_subnets } ] } # Route53 Hosted Zone module \u0026#34;zones\u0026#34; { source = \u0026#34;terraform-aws-modules/route53/aws//modules/zones\u0026#34; version = \u0026#34;~\u0026gt; 1.0\u0026#34; zones = { (var.domain_name) = { } } } # IAM Role Service Account for the cert-manager module \u0026#34;cert_manager_irsa\u0026#34; { source = \u0026#34;terraform-aws-modules/iam/aws//modules/iam-assumable-role-with-oidc\u0026#34; version = \u0026#34;3.6.0\u0026#34; create_role = true role_name = \u0026#34;${var.name}-cert_manager-irsa\u0026#34; provider_url = replace(module.eks.cluster_oidc_issuer_url, \u0026#34;https://\u0026#34;, \u0026#34;\u0026#34;) role_policy_arns = [aws_iam_policy.cert_manager_policy.arn] oidc_fully_qualified_subjects = [\u0026#34;system:serviceaccount:cert-manager:cert-manager\u0026#34;] } resource \u0026#34;aws_iam_policy\u0026#34; \u0026#34;cert_manager_policy\u0026#34; { name = \u0026#34;${var.name}-cert-manager-policy\u0026#34; path = \u0026#34;/\u0026#34; description = \u0026#34;Policy, which allows CertManager to create Route53 records\u0026#34; policy = jsonencode({ \u0026#34;Version\u0026#34; : \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34; : [ { \u0026#34;Effect\u0026#34; : \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34; : \u0026#34;route53:GetChange\u0026#34;, \u0026#34;Resource\u0026#34; : \u0026#34;arn:aws:route53:::change/*\u0026#34; }, { \u0026#34;Effect\u0026#34; : \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34; : [ \u0026#34;route53:ChangeResourceRecordSets\u0026#34;, \u0026#34;route53:ListResourceRecordSets\u0026#34; ], \u0026#34;Resource\u0026#34; : \u0026#34;arn:aws:route53:::hostedzone/${local.route53_zone_id}\u0026#34; }, ] }) } output \u0026#34;route53_zone_id\u0026#34; { value = local.route53_zone_id } output \u0026#34;cert_manager_irsa_role_arn\u0026#34; { value = module.cert_manager_irsa.this_iam_role_arn } terraform init terraform apply -var \u0026#34;name=cert-test\u0026#34; -var \u0026#34;domain_name={{your-domain-name}}\u0026#34; Running this Terraform module will create the kubeconfig in the kubeconfig_{{name}}-eks file. Set the KUBECONFIG environment variable to it:\nexport KUBECONFIG=\u0026#34;$PWD/kubeconfig_{{name}}-eks\u0026#34; The EKS OIDC provider in AWS IAM created to support IAM roles for Kubernetes service accounts\nDeploy an ingress controller To deploy the ingress controller I will use ingress-nginx.\nhelm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx helm install ingress-nginx ingress-nginx/ingress-nginx \\ --create-namespace \\ --namespace=\u0026#34;ingress-nginx\u0026#34; This is pretty straightforward and does not require any special configuration for the certificates to work. AWS will create a Classic Load Balancer for the ingress controller.\nDeploy Cert Manager Now let\u0026rsquo;s deploy the CertManager. It will use DNS01 challenges with Route53 to verify, that we are the owner of the domain name. We have to configure two things here:\nIAM role used by the service account to make calls to the AWS API, set the challenge type to DNS01 and use the Route53 Hosted Zone for it The IAM role is configured using an annotation on the ServiceAccount Kubernetes resource. We will use this Helm chart and using it we can set the annotation with the following values for the Helm chart:\n#cert-manager-values.yml serviceAccount: annotations: eks.amazonaws.com/role-arn: {{cert-manager-iam-role-arn}} installCRDs: true # the securityContext is required, so the pod can access files required to assume the IAM role securityContext: enabled: true fsGroup: 1001 Replace the {{cert-manager-iam-role-arn}} with our IAM role ARN from the Terraform output. It should be in the form: arn:aws:iam::{{AWS_ACCOUNT_ID}}:role/cert-test-cert_manager-irsa. Now run the Helm chart:\nhelm repo add jetstack https://charts.jetstack.io helm upgrade cert-manager jetstack/cert-manager \\ --install \\ --namespace cert-manager \\ --create-namespace \\ --values \u0026#34;cert-manager-values.yml\u0026#34; \\ --wait Now we have to create an Issuer for the CertManager. We will use Let\u0026rsquo;s Encrypt for issuing the certificates for our services. Here we also configure the challenge type, which will be used for issuing the certificates. Fill the template below with the parameters you got from the Terraform output and create the resource:\n# cert-issuer.yml apiVersion: cert-manager.io/v1beta1 kind: ClusterIssuer metadata: name: letsencrypt spec: acme: email: {{your-email}} privateKeySecretRef: name: letsencrypt server: https://acme-v02.api.letsencrypt.org/directory solvers: - dns01: route53: region: {{your-aws-region}} hostedZoneID: {{your-hosted-zone-id}} kubectl apply -f cert-issuer.yml Now your cluster is ready, and you can start getting certificates for your Ingress resources.\nTest the setup To test our configuration we will deploy DokuWiki on our Kubernetes cluster. We have to set an annotation on the Ingress resource to tell CertManager, which Issuer should be used to get the certificate.\nCreate the following file for the Helm chart values and then run the chart:\n# by default DokuWiki creates an LoadBalancer service and we do not need this service: type: ClusterIP ingress: enabled: true hostname: {{you-dokuwiki-domain}} certManager: true tls: true annotations: cert-manager.io/cluster-issuer: letsencrypt # use the letsencrypt ClusterIssuer helm repo add bitnami https://charts.bitnami.com/bitnami helm install dokuwiki bitnami/dokuwiki \\ --namespace default \\ --values dokuwiki-values.yml The last thing is to add an A ALIAS record for the Dokuwiki domain name pointing to the ingress Classic Load Balancer in the Route53 Hosted Zone.\nCreating an A ALIAS\nAfter the new DNS entry propagates you should be able to access the domain and see DokuWiki with a Let\u0026rsquo;s Encrypt signed SSL certificate!\n$ kubectl get certificate -n default NAME READY SECRET AGE doku.eks.myhightech.org-tls True doku.eks.myhightech.org-tls 23m Dokuwiki with an Let\u0026rsquo;s Encrypt signed SSL certificate\nRead more https://cert-manager.io/docs/configuration/acme/dns01/route53/ https://cert-manager.io/docs/usage/ingress/ https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html https://docs.aws.amazon.com/eks/latest/userguide/best-practices-security.html ","permalink":"https://myhightech.org/posts/20210402-cert-manager-on-eks/","summary":"Integrating Cert Manager with Route53 on EKS In this article I will show, how you can automatically get Let\u0026rsquo;s Encrypt SSL certificates using Cert Manager. We will leverage the DNS01 challenge and use a Route53 Hosted Zone to answer the challenge. The Cert Manager will use an EKS IAM Role Service Account, which follows AWS best practices for security.\nSet up the EKS cluster with Terraform We will use this EKS module to provision our EKS cluster.","title":"Integrating Cert Manager with Route53 on EKS"},{"content":" I was missing a addon manager for ESO UI on Linux. There is Minion, which I tried to run in the Wine prefix of my ESO installation, but it crashed too often.\nWhat I expected from the addon manager was:\nworking on Linux command line interface easy to backup the addon configuration So I created eso-addons. It uses a single configuration file for storing the addon configuration, so I can just backup this config file and use it to restore my addons. I prebuilt binaries for Linux, Windows and MacOS, but I didn\u0026rsquo;t test the Windows and MacOS ones.\nYou can check my config here on GitHub.\n","permalink":"https://myhightech.org/posts/20201228-eso-addon-manager/","summary":"I was missing a addon manager for ESO UI on Linux. There is Minion, which I tried to run in the Wine prefix of my ESO installation, but it crashed too often.\nWhat I expected from the addon manager was:\nworking on Linux command line interface easy to backup the addon configuration So I created eso-addons. It uses a single configuration file for storing the addon configuration, so I can just backup this config file and use it to restore my addons.","title":"My simple The Elder Scrolls Online addon manager in Rust"},{"content":"In some shader heavy OpenGL and Vulkan games you can experience shuttering, when running the game for the first time. An example is Overwatch or Path of Exile.\nWhat\u0026rsquo;s happening in the first minutes of the game is shader cache pre-compilation. It\u0026rsquo;s a process of compiling shader code into GPU instructions .The final cache code is dependent on the specific GPU you have, so this cannot be baked into the game files.\nAs said above, normally the cache is built, when running the game, but in Steam you can trigger it manually. In my case it removed the shuttering in Path of Exile.\nTo perform it launch Steam from the terminal using:\nsteam -console This will open the Steam client with a new menu option next to your profile name: There you can use the following command to trigger a shader cache pre-compilation:\nshader_build \u0026lt;app_id\u0026gt; shader_build 238960 # for Path of Exile The cache is stored in .steam/steam/steamapps/shadercache/\u0026lt;app_id\u0026gt;. Watching the overall size of the directory can tell you, if the shaders were compiled. For Path of Exile I have currently 185 MB of cache files. I had to launch the game and play a few minutes, before shader_build was able to build the cache shader files.\n$ pwd /home/damian/.steam/steam/steamapps/shadercache/238960 $ du -lh 164M\t./nvidiav1/GLCache/7dccdb8912afcd6839eeade90f7db1fc/940801954b37b742 164M\t./nvidiav1/GLCache/7dccdb8912afcd6839eeade90f7db1fc 164M\t./nvidiav1/GLCache 164M\t./nvidiav1 8,0K\t./DXVK_state_cache 22M\t./fozpipelinesv4 4,0K\t./fozmediav1 185M\t. ","permalink":"https://myhightech.org/posts/20201031-steam-shader-precompile/","summary":"In some shader heavy OpenGL and Vulkan games you can experience shuttering, when running the game for the first time. An example is Overwatch or Path of Exile.\nWhat\u0026rsquo;s happening in the first minutes of the game is shader cache pre-compilation. It\u0026rsquo;s a process of compiling shader code into GPU instructions .The final cache code is dependent on the specific GPU you have, so this cannot be baked into the game files.","title":"Pre-compile shader cache in Steam. How to improve performance for some games"},{"content":" Is gaming on Linux possible? tldr; Yes, it is! In many cases the user experience is like on Windows!\nThe biggest issue I had with switching completely to a Linux based OS were games. Game studios do not really care about Linux support, as the market share for this OS is around 2%. Moreover, OpenGL, the graphics library supported in Linux, is not developer friendly, so they did not want to use it. Of course, there are some games with a native port, like the Total Wars and Tomb Raider series, but this is a minority.\nGamers tried to use compatibility layers like Wine or CrossOver to run Windows applications, but sometimes it did not work or the performance loss was too big. Thankfully, things changed with the creation of Vulkan and Valve\u0026rsquo;s forked Wine: Proton. In the meantime, an application called Lutris appeared and thanks to the Linux gamer community, more and more games can be run on it with a satisfactory performance, sometimes even better!\nIn this article I would like to show you, how I play on Linux and present the different tools I use. At the end, you will see my benchmarks from different games, so you can develop your own opinion on this.\nWine Is Not an Emulator The story started in 1993, when Bob Amstadt and Eric Youngdale released the first version of Wine. It is an compatibility layer for Microsoft Windows, which allows to run Windows binaries on Unix-like operating systems. What it does, it translates the Windows system calls to Unix system calls and provides various components like the registry, the directory structure and system libraries. You can think about the amount of reverse-engineering, which has been done, to create this piece of software, as Windows source code is not available publicly.\nWine uses something called Wine prefixes to create separate Wine configurations for the Windows components. You could ask: \u0026ldquo;Why would I need that? When I use Windows I only have one OS installation on my hard drive?\u0026rdquo;. Right, but sometimes you can have conflicts between different DLL libraries or specific registry configuration. In Wine it more often became a problem, as it\u0026rsquo;s only a reverse-engineered solution to create a runtime for Windows binaries. Often people had to use a specific version of some DLLs to run an applications and prefixes allow to create an isolated environment for these.\nWine is not focused on games, although it also supports translating DirectX calls into OpenGL. By the time I tried to use it for games in 2012 the results were not as great. It was hard to configure Wine and the prefixes, you had to manually add some DLLs or install some additional stuff like .NET frameworks. There was PlayOnLinux, which had scripts for many games and configured the Wine prefixes for you, but still, there were too many problems for me and I stayed on Windows.\nWhat is Proton? Things changed a lot with Valve\u0026rsquo;s contribution to the Linux community in 2017: Proton and Steam Play. According to Wikipedia, \u0026ldquo;Proton is a compatibility layer for Microsoft Windows games to run on Linux-based operating systems\u0026rdquo;. Sound like Wine, right? In fact, Proton is a project forked from Wine, but focuses more on 3D graphics and sound libraries used in games. For example, it contains dxvk, which translates DirectX 9, 10 and 11 to Vulkan calls. Unfortunately, DirectX 12 is currently not supported, but there is work ongoing on vkd3d, which should bring DirectX 12 support.\nProton is integrated with Steam through Steam Play. It allows for a native-like experience for playing Steam games available only for Windows. Valve doesn\u0026rsquo;t guarantee games will work, but from my experience most of the games do. A few examples are Skyrim, Divinity: Original Sin 2 and Monster Train, which was playable right from the release day.\nOn the webpage ProtonDB you can check user reviews and ratings for the performance of games. Witcher 3, Skyrim and Elder Scrolls Online have the Platinum rating, which means they run perfectly out of the box with no or small differences compared to Windows. I use that page quite often before buying a new game, to verify, if I will be able to play it. It\u0026rsquo;s also helpful to check the reviews, as people write, what\u0026rsquo;s working, what\u0026rsquo;s not and how to optimize the game.\nGlorious Eggroll Even though Proton and SteamPlay is a huge improvement over Wine and it raised the gamer experience to an near Windows level, can be issues in some games. Valve focuses on stability and isn\u0026rsquo;t shipping the latest Wine in Steam Play (currently the newest Steam included is 5.13-1), but as Proton is open source, the community is making their own patches and builds. The most popular build is GloriousEggroll\u0026rsquo;s Proton build. It\u0026rsquo;s basically a Proton build with additional patches for some games and newer version of the components used in Proton.\nInstalling a new Proton build is very easy. You have to download the release package and put in $HOME/.steam/root/compatibilitytools.d/. It will automatically be available to select from a dropdown in Steam. As I\u0026rsquo;m too lazy for this, I have written a Python script to manage the GE builds. :P\nWhy would you use the GE build? In some games the Steam included Proton has issues. I had to use Proton-5.9-GE-7-ST for Borderlands 3, because the default one does not support Media Foundation for now, which was required to play some in-game videos. You can get a frame rate increase of a few percent in some cases.\nCurrently on my PC I have:\nSteam included Proton builds Proton-5.9-GE-8-ST Proton-5.11-GE-3-MF Proton-5.6-GE-2 When running a new game, I always start with the newest Steam Proton and change only, if there are issues. In like 80% of cases \u0026ldquo;it just works™\u0026rdquo;.\nLutris OK, Steam is a huge game library, but what about other launchers, like Battle.net, EA Origin, Epic Store Games? For this I have Lutris. If you remember PlayOnLinux, then Lutris is very similar. It\u0026rsquo;s basically a manager and automated installer for many games.\nAfter you installed it, you can open the Lutris page for the game (let\u0026rsquo;s take ESO as example). There you can select one of the prepared installation modes (it could be a standalone installation, through Steam, Battle.net or other launchers). Basically people are writting installation scripts, which install the launcher, then the game, may preconfigure the Wine prefix somehow. In my experience it is not so polished like SteamPlay, but for popular games (like ESO or Overwatch) the scripts are well prepared and work out of the box. There is a comment section under each game, so if something\u0026rsquo;s wrong, you can look there for help.\nLutris also supports custom Proton builds and uses the Steam directory in $HOME/.steam/root/compatibilitytools.d/, so you can use the same Proton builds in Steam and Lutris. In this case I also prefer the default Lutris Proton and use the GE only, if there are problems.\nThrough Lutris I\u0026rsquo;m playing:\nElder Scrolls Online Minecraft Battle.net games: Overwatch and Diablo III Special case: Epic Store Games. For this I use https://github.com/derrod/legendary. It\u0026rsquo;s a command line ESG replacement and works very good IMO:\n# Capnip is the alias for Borderlands 3 in legendary $ legendary install Capnip # install Borderlands 3 $ legendary launch Capnip # run Borderlands 3 Some benchmarks and comparison To show you the differences in performance, I did a few benchmarks. The graphs compare the framerate, CPU and GPU load in Linux and Windows. For measuring the FPS I used MangoHUD on Linux and MSI Afterburner on Windows.\nMy PC specs are:\nMobo: MSI B450M MORTAR MAX CPU: Ryzen 7 3700X GPU: Palit GeForce RTX 2070 SUPER JetStream 8GB RAM: G.Skill Trident Z RGB, 32GB, 3600 MHz, CL16 OS: Windows 10 and Pop_OS! 20.04 Elder Scrolls Online, Ultra, 3440x1440p I\u0026rsquo;m playing ESO on Lutris. It works without problems, there is a significant frame drop of around 20-30%, but the game is still enjoyable.\nAssassins Creed: Odyssey, Ultra, 3440x1440p This one was surprising for me! On my PC the built-in benchmark shows 10% higher framerates on Linux! I verified the graphics setting and they were the same on both OSes. I\u0026rsquo;m not really sure how this is possible.\nShadow of the Tomb Raider, Ultra, 3440x1440p This one has actually a native Linux port. When playing, I get a few more frames compared to running on Windows with DirectX 11. What I noticed here, the available display settings are different in each systems. On Linux there are no HDR and stereoscopy. IMO, nothing really important.\nDiablo III, Highest, 3440x1440p Works without any problems, with a 20% FPS loss to Windows. I installed Battle.net through Lutris and then Diablo III in the launcher.\nOverwatch, Epic, 3440x1440p Like Diablo III, I installed it using Lutris. The performance is slightly better on Windows.\nOne thing to note is, that when you start the game it\u0026rsquo;s going to compile some shaders and during that, the performance is much lower and there is a lot of clipping. You have to wait a few minutes, till this completes.\nCompany of Heroes 2, Highest, 3440x1440p This game has a few problems, but is playable. It has a Linux native version, but you can\u0026rsquo;t play multiplayer with Windows gamers as the version number is different, so I had to use SteamPlay. I also had to set the audio quality to low, as it was clipping. The game sometimes crashed for me at the begging of a match, which is annoying in multiplayer.\nThe framerate loss is huge, around 50%, but it\u0026rsquo;s above 30 FPS, so playable for an RTS game.\nDOOM, Ultra, 3440x1440p Although DOOM hasn\u0026rsquo;t a native port, it uses Vulkan as the graphics library, which explains comparable framerates on Windows and Linux. Playable without any problems.\nWarhammer 2: Total War This has a native Linux port. Performance in the benchmark is similar, with a slight advantage on Windows.\nOverall experience In general I\u0026rsquo;m satisfied with gaming on Linux. Valve did a very good job with Proton and SteamPlay, which accelerated the progress in improving the overall gamer experience. In addition with Proton GE and the Lutris launcher I have a very nice gaming station based on Pop_OS!. This distribution has Nvidia graphics drivers in the repository, so installing them is easy. The performance is not the same like on Windows, the gap is in most cases a few percent, but that\u0026rsquo;s OK for me.\nStill, not everything is working. If you looks at the \u0026ldquo;Fix Wanted\u0026rdquo; list on ProtonDB, you will see popular games like Valorant, Rainbox Six Siege, Destiny 2, PUBG and Black Desert not working. Recently GTA V broke and it was not playable for a few days. :(\nSome other pain points I see:\nsome anti-cheat systems do not work, especially those, which are working on the kernel level. EAC and BattleEye are not supported in Wine. Riot announced it plans to introduce a kernel-level anti-cheat system in League of Legends. In this case, without support of the developer, I don\u0026rsquo;t think we can do much. Putting the compatibility issue aside: I personally don\u0026rsquo;t like the idea of such low-level anti-cheat systems missing \u0026ldquo;gamer\u0026rdquo; software - overclocking/motherboard applications, controlling RGB, etc. For streaming there is OBS. For FPS meter there is MangoHUD. Still, CPU, GPU, motherboard manufacturers don\u0026rsquo;t port their apps to Linux and it\u0026rsquo;s hard to control the RGB or overclock your system. Personally, I don\u0026rsquo;t think they will, because the user base is too small. Hopefully the community will come up will some programs for that. Edit 23.10.2020: for overclocking there is CoreCtrl. Thanks beko for pointing me to the tool!\nIf you want to try to switch to Linux, I recommend joining the Discord channel GamingOnLinux (https://discord.gg/PH66Gm) and following https://www.gamingonlinux.com/. You can seek there for help or find new mates for multiplayer games.\nRead more List of Proton compatible games on Steam Linux game benchmarks ProtonDB Lutris webpage Glorious Eggroll Proton repository ","permalink":"https://myhightech.org/posts/20201012-gaming-on-linux/","summary":"Is gaming on Linux possible? tldr; Yes, it is! In many cases the user experience is like on Windows!\nThe biggest issue I had with switching completely to a Linux based OS were games. Game studios do not really care about Linux support, as the market share for this OS is around 2%. Moreover, OpenGL, the graphics library supported in Linux, is not developer friendly, so they did not want to use it.","title":"Gaming on Linux? Is it possible in 2020?"},{"content":"Static website hosting You most probably heard about static website hosting, static website generators and Github Pages or Netlify. After the boom of web technologies the backends got heavier and heavier, page loading times increased multiple times. You wanted to create a blog or portfolio website and you had to:\nbuy a server/VPS buy/install a database buy a domain install the required backend software and configure it ensure the system is patched make backups You either made this by yourself or bought a managed solution from some provider.\nWhen the user interaction with the website is limited only to reading and only you are providing content for the website such solutions are a overkill. Inspired by the old times, when pages were only a bunch of HTML and CSS code, people created tools, where you can write the content you want to present, pick or create a theme and this gets compiled to HTML, CSS and (sometimes) Javascript.\nYou can store the code for your page in git, so backups are easy. As there is no server-side code execution or database, the surface for potential cyberattack is small. In this post I would like to show you, how I created this blog and hosted it using AWS Amplify. The repository for the project is hosted on GitHub\nHugo - static site generator To manage the content and build the website I use Hugo. It\u0026rsquo;s a static site generator written in Golang and allows to write content in Markdown or HTML. It also has an theme engine, so you can use themes created by other people, extend them or prepare your own theme.\n# create the project $ hugo new site my-page $ cd my-page $ git init # add \u0026#39;hugo-developer-portfolio\u0026#39; theme $ git submodule add https://github.com/adityatelange/hugo-PaperMod themes/papermod echo \u0026#39;theme = \u0026#34;papermoc\u0026#34;\u0026#39; \u0026gt;\u0026gt; config.toml After you initialize the project you\u0026rsquo;ll end up with a few directories and files:\nconfig.toml - Hugo configuration file. Here you defined the Hugo configuration, theme and theme parameters. You can find more information here content - your page content, posts, etc. If you want to create a new page on your site or write a new blog post, you\u0026rsquo;ll do this here themes - directory for themes static - directory for static content like images, videos, which will be served from the root path. Check here for more details layouts - directory for layouts for the pages. You store here templates for parts of the page, which Hugo fills with the content and combines together. In most cases you will use the layouts from the theme, but you can create new or override the theme provided layouts You can find more information about the structure of the project here.\nIn my case I just picked the PaperMod, edited the template for the post (I wanted the share buttons on top) and adjusted the config.toml.\nYou can run a local server with hot-reload using\nhugo server To build the final static files run:\nhugo This will create a directory public with all the output files, which can be uploaded to a static website hosting.\nHosting using AWS Amplify AWS Amplify is a framework to build web and mobile apps, similar to Google Firebase. In my case I just used it\u0026rsquo;s ability to host static sites and provide a simple CI/CD pipeline to deploy new static files in case a new commit is made.\nSo I went through this guide and created my Amplify app in AWS Console. Amplify automatically detected, that the repository contains an hugo project and created the buildspec.yml for me.\nRight after was the pipeline started and after a few minutes my page was available.\nI also added the domain I have registered in Route 53: I also use the preview feature in Amplify. If a new PR is created in the source git repository, then Amplify deploys the code in a temporary environment, so you can check, if everything looks ok. What\u0026rsquo;s neat, this is visible in GitHub as a PR check and you have there the link to the environment.\nSummary I\u0026rsquo;m pretty happy with the experiance Hugo and Amplify provided. After you tweak the configuration of your Hugo project, you just need to write the content in Markdown and that\u0026rsquo;s all. Amplify was a few-click-and-forget My current workflow looks following:\nMake a branch Write the post Create a PR Check the preview environment in Amplify Merge the PR Check the post is available on the blog I don\u0026rsquo;t have to worry about any server maintenance, patching, database backups. The price is also low compared to buying a server ($0.15 per GB served), although I can imagine it could skyrocket, if you serve for eg. videos or large images and have a large user base.\nRead more Hugo documentation AWS Amplify Console documentation AWS Amplify documentation Best static website hostings on Slant ","permalink":"https://myhightech.org/posts/build-page-with-hugo/","summary":"Static website hosting You most probably heard about static website hosting, static website generators and Github Pages or Netlify. After the boom of web technologies the backends got heavier and heavier, page loading times increased multiple times. You wanted to create a blog or portfolio website and you had to:\nbuy a server/VPS buy/install a database buy a domain install the required backend software and configure it ensure the system is patched make backups You either made this by yourself or bought a managed solution from some provider.","title":"Build a static web page with Hugo and Amplify"},{"content":"AWS Serverless Application Model I was recently preparing for my AWS DevOps Engineer exam and I wanted to give AWS Serverless Application Model a try. AWS Serverless Application Model is a framework to build and deploy serverless application on AWS using Lambda, API Gateway and DynamoDB.\nUnder the hood it is a AWS CloudFormation transform, which expands the CloudFormation syntax and adds additional resources under the AWS::Serverless namespace. During the template provisioning those resources get expanded to basic CloudFormation resources.\nThe sam CLI has commands to build, test, package and deploy your serverless applications. It makes deploying serverless application much easier than packaging your application by yourself and using generic CloudFormation templates.\nInitialize the project To initialize the project you use the sam init command. It will ask a few questions and prepare a boilerplate project.\n$ sam init Which template source would you like to use? 1 - AWS Quick Start Templates 2 - Custom Template Location Choice: 1 Which runtime would you like to use? 1 - nodejs12.x 2 - python3.8 3 - ruby2.7 4 - go1.x 5 - java11 6 - dotnetcore3.1 7 - nodejs10.x 8 - python3.7 9 - python3.6 10 - python2.7 11 - ruby2.5 12 - java8.al2 13 - java8 14 - dotnetcore2.1 Runtime: 4 Project name [sam-app]: Cloning app templates from https://github.com/awslabs/aws-sam-cli-app-templates.git AWS quick start application templates: 1 - Hello World Example 2 - Step Functions Sample App (Stock Trader) Template selection: 1 ----------------------- Generating application: ----------------------- Name: sam-app Runtime: go1.x Dependency Manager: mod Application Template: hello-world Output Directory: . Next steps can be found in the README file at ./sam-app/README.md $ tree . ├── Makefile ├── README.md ├── hello-world │ ├── go.mod │ ├── main.go │ └── main_test.go └── template.yaml 1 directory, 6 files The project has a CloudFormation template in template.yaml and an example Lambda function handler in hello-world/main.go. You can notice the Transform: AWS::Serverless-2016-10-31 field in the template, which means we are using AWS SAM:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 AWSTemplateFormatVersion: \u0026#39;2010-09-09\u0026#39; Transform: AWS::Serverless-2016-10-31 [...] Resources: HelloWorldFunction: Type: AWS::Serverless::Function Properties: CodeUri: hello-world/ Handler: hello-world Runtime: go1.x Tracing: Active Events: CatchAll: Type: Api Properties: Path: /hello Method: GET Environment: Variables: PARAM1: VALUE [...] Local development Now to build the Lambda deployment packages run sam build.\n$ sam build Building function \u0026#39;HelloWorldFunction\u0026#39; Running GoModulesBuilder:Build Build Succeeded Built Artifacts : .aws-sam/build Built Template : .aws-sam/build/template.yaml Commands you can use next ========================= [*] Invoke Function: sam local invoke [*] Deploy: sam deploy --guided A nice feature of SAM CLI is that you can run the Lambda function locally or even run a server, which simulates whole API in the template. For this you will need to have Docker installed.\nTo invoke a single function use sam local invoke \u0026lt;function-name\u0026gt;: $ sam local invoke HelloWorldFunction Invoking hello-world (go1.x) Failed to download a new amazon/aws-sam-cli-emulation-image-go1.x:rapid-1.1.0 image. Invoking with the already downloaded image. Mounting /home/damian/Projects/sam-test/sam-app/.aws-sam/build/HelloWorldFunction as /var/task:ro,delegated inside runtime container START RequestId: 861e0e67-1567-104f-55ba-5ee5b7c63eeb Version: $LATEST END RequestId: 861e0e67-1567-104f-55ba-5ee5b7c63eeb REPORT RequestId: 861e0e67-1567-104f-55ba-5ee5b7c63eeb Init Duration: 49.63 ms Duration: 549.73 ms Billed Duration: 600 ms Memory Size: 128 MB Max Memory Used: 51 MB {\u0026#34;statusCode\u0026#34;:200,\u0026#34;headers\u0026#34;:null,\u0026#34;multiValueHeaders\u0026#34;:null,\u0026#34;body\u0026#34;:\u0026#34;Hello, 178.43.131.97\\n\u0026#34;}\nYou can also start a server, which simulates the AWS API Gateway: $ sam local start-api Mounting HelloWorldFunction at http://127.0.0.1:3000/hello [GET] You can now browse to the above endpoints to invoke your functions. You do not need to restart/reload SAM CLI while working on your functions, changes will be reflected instantly/automatically. You only need to restart SAM CLI if you update your AWS SAM template 2020-09-17 17:54:42 * Running on http://127.0.0.1:3000/ (Press CTRL+C to quit) # in other terminal $ curl http://127.0.0.1:3000/hello Hello, 178.43.131.97\nDeploy the application To deploy Lambda functions you need to package and upload your code to S3. AWS SAM will handle this for you, but it requires an additonal configuration file samconfig.toml to know what bucket it should use.\n1 2 3 4 5 6 7 8 9 10 11 # samconfig.toml version = 0.1 [default] [default.deploy] [default.deploy.parameters] stack_name = \u0026#34;sam-app\u0026#34; s3_bucket = \u0026#34;aws-sam-cli-managed-default-samclisourcebucket-128t3n8a6nlhy\u0026#34; s3_prefix = \u0026#34;sam-app\u0026#34; region = \u0026#34;eu-west-1\u0026#34; confirm_changeset = true capabilities = \u0026#34;CAPABILITY_IAM\u0026#34; You can either provision the S3 bucket and create the samconfig.toml file manually or use the --guided flag in the sam deploy command, so SAM will create it for you.\nWhat sam deploy does is:\nDetect, which functions must be updated Upload the CloudFormation template and Lambda deployment packages to S3 Generate the ChangeSet and deploy it $ sam deploy --guided Configuring SAM deploy ====================== Looking for samconfig.toml : Not found Setting default arguments for \u0026#39;sam deploy\u0026#39; ========================================= Stack Name [sam-app]: AWS Region [us-east-1]: eu-west-1 #Shows you resources changes to be deployed and require a \u0026#39;Y\u0026#39; to initiate deploy Confirm changes before deploy [y/N]: y #SAM needs permission to be able to create roles to connect to the resources in your template Allow SAM CLI IAM role creation [Y/n]: Y HelloWorldFunction may not have authorization defined, Is this okay? [y/N]: y Save arguments to samconfig.toml [Y/n]: Looking for resources needed for deployment: Not found. Creating the required resources... Successfully created! Managed S3 bucket: aws-sam-cli-managed-default-samclisourcebucket-128t3n8a6nlhy A different default S3 bucket can be set in samconfig.toml Saved arguments to config file Running \u0026#39;sam deploy\u0026#39; for future deployments will use the parameters saved above. The above parameters can be changed by modifying samconfig.toml Learn more about samconfig.toml syntax at https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/serverless-sam-cli-config.html Uploading to sam-app/4a708650ed51c1f21e152bcc6440cf00 1325 / 1325.0 (100.00%) Deploying with following values =============================== Stack name : sam-app Region : eu-west-1 Confirm changeset : True Deployment s3 bucket : aws-sam-cli-managed-default-samclisourcebucket-128t3n8a6nlhy Capabilities : [\u0026#34;CAPABILITY_IAM\u0026#34;] Parameter overrides : {} Initiating deployment ===================== HelloWorldFunction may not have authorization defined. Uploading to sam-app/cbc7be7eaba81f76b3cd7e012f847498.template 1155 / 1155.0 (100.00%) Waiting for changeset to be created.. CloudFormation stack changeset --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- Operation LogicalResourceId ResourceType --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- + Add HelloWorldFunctionCatchAllPermissionProd AWS::Lambda::Permission + Add HelloWorldFunctionRole AWS::IAM::Role + Add HelloWorldFunction AWS::Lambda::Function + Add ServerlessRestApiDeployment47fc2d5f9d AWS::ApiGateway::Deployment + Add ServerlessRestApiProdStage AWS::ApiGateway::Stage + Add ServerlessRestApi AWS::ApiGateway::RestApi --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- Changeset created successfully. arn:aws:cloudformation:eu-west-1:146986152083:changeSet/samcli-deploy1600354796/6dc83be8-7e11-48c9-b6b0-8df847a66ac8 Previewing CloudFormation changeset before deployment ====================================================== Deploy this changeset? [y/N]: y 2020-09-17 17:00:07 - Waiting for stack create/update to complete CloudFormation events from changeset ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- ResourceStatus ResourceType LogicalResourceId ResourceStatusReason ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- CREATE_IN_PROGRESS AWS::IAM::Role HelloWorldFunctionRole - CREATE_IN_PROGRESS AWS::IAM::Role HelloWorldFunctionRole Resource creation Initiated CREATE_COMPLETE AWS::IAM::Role HelloWorldFunctionRole - CREATE_IN_PROGRESS AWS::Lambda::Function HelloWorldFunction - CREATE_COMPLETE AWS::Lambda::Function HelloWorldFunction - CREATE_IN_PROGRESS AWS::Lambda::Function HelloWorldFunction Resource creation Initiated CREATE_IN_PROGRESS AWS::ApiGateway::RestApi ServerlessRestApi - CREATE_COMPLETE AWS::ApiGateway::RestApi ServerlessRestApi - CREATE_IN_PROGRESS AWS::ApiGateway::RestApi ServerlessRestApi Resource creation Initiated CREATE_IN_PROGRESS AWS::ApiGateway::Deployment ServerlessRestApiDeployment47fc2d5f9d - CREATE_IN_PROGRESS AWS::Lambda::Permission HelloWorldFunctionCatchAllPermissionProd Resource creation Initiated CREATE_IN_PROGRESS AWS::Lambda::Permission HelloWorldFunctionCatchAllPermissionProd - CREATE_COMPLETE AWS::ApiGateway::Deployment ServerlessRestApiDeployment47fc2d5f9d - CREATE_IN_PROGRESS AWS::ApiGateway::Deployment ServerlessRestApiDeployment47fc2d5f9d Resource creation Initiated CREATE_IN_PROGRESS AWS::ApiGateway::Stage ServerlessRestApiProdStage - CREATE_IN_PROGRESS AWS::ApiGateway::Stage ServerlessRestApiProdStage Resource creation Initiated CREATE_COMPLETE AWS::ApiGateway::Stage ServerlessRestApiProdStage - CREATE_COMPLETE AWS::Lambda::Permission HelloWorldFunctionCatchAllPermissionProd - CREATE_COMPLETE AWS::CloudFormation::Stack sam-app - ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- CloudFormation outputs from deployed stack --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- Outputs --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- Key HelloWorldFunctionIamRole Description Implicit IAM Role created for Hello World function Value arn:aws:iam::146986152083:role/sam-app-HelloWorldFunctionRole-URQ6NEJVAE9X Key HelloWorldAPI Description API Gateway endpoint URL for Prod environment for First Function Value https://xzv8lq1611.execute-api.eu-west-1.amazonaws.com/Prod/hello/ Key HelloWorldFunction Description First Lambda Function ARN Value arn:aws:lambda:eu-west-1:146986152083:function:sam-app-HelloWorldFunction-DMK403RR7HEW --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- Successfully created/updated stack - sam-app in eu-west-1 SAM created the following CloudFormation stacks. aws-sam-cli-managed-default stack was created, because we used --guided and SAM provisioned the missing S3 bucket. sam-app stack is the actual serverless application\nThe AWS::Serverless::* resources were transformed into other CloudFormation resources. SAM also implicitly created an API Gateway for us\nEnhance the template and add canary deployment The nice thing in AWS SAM is, that it\u0026rsquo;s just an extension of CloudFormation templates, so you can define other resources, export output values or reference values in other stacks. It also gives an option to define the deployment strategy for Lambda functions and set triggers for rollbacks. You can perform canary deployment on the AWS Lambda level by using Lambda aliases. Let\u0026rsquo;s change the HelloWorld function, add DeploymentPreference and AutoPublishAlias parameters and define all CloudWatch alarm to rollback the deployment in case your function does not work during the CodeDeploy AllowTraffic phase.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 # template.yaml [...] Resources: HelloWorldServerErrorAlarm: Type: AWS::CloudWatch::Alarm Properties: AlarmName: HelloWorldServerErrorAlarm EvaluationPeriods: 1 Metrics: - Id: m1 MetricStat: Metric: Dimensions: - Name: FunctionName Value: !Ref HelloWorldFunction MetricName: Errors Namespace: AWS/Lambda Period: !!int 60 Stat: Average ComparisonOperator: GreaterThanThreshold Threshold: 0.05 TreatMissingData: notBreaching HelloWorldFunction: Type: AWS::Serverless::Function Properties: CodeUri: hello-world/ Handler: hello-world Runtime: go1.x Tracing: Active AutoPublishAlias: live DeploymentPreference: Type: Canary10Percent5Minutes Alarms: - !Ref HelloWorldServerErrorAlarm Events: CatchAll: Type: Api Properties: Path: /hello Method: GET [...] After defining DeploymentPreference SAM creates an CodeDeploy application with a deployment group for every Lambda function. sam deploy will upload the Lambda deployment packages to S3 and trigger an CodeDeploy deployment to release the new Lambda function version\nNow, let\u0026rsquo;s make a small change in the source code, rebuild the package and deploy it. This can take a bit, cause the command waits, till the CodeDeploy deployment is finished:\n$ sam build ... $ sam deploy ... Successfully created/updated stack - sam-app in eu-west-1 During the sam deploy a CodeDeploy deployment is triggered\nCodeDeploy performs the canary release using an Lambda alias. Here we can see, that 10% of the requests are directed to the 4 version of the Lambda, while the rest goes to version 3\nMore advanced example A more advanced example is available on my Github aws-dev-ops-preparation repo. It shows also how to use and implement an lifecycle hook function to validate the deployment.\nAdditional notes about SAM A few things I would like to mention, because I lost a few hours by not knowing them:\nHook functions must start with prefix CodeDeployHook_ or you have to provide an custom IAM role for the CodeDeploy in DeploymentPreference The hook functions must call the AWS API codedeploy:PutLifecycleEventHookExecutionStatus to inform CodeDeploy, if the hook passed or failed. In other case it will wait for 1 hour and fail The Alarms in DeploymentPreference can be used to rollback the deployment on Cloudwatch Alarm. The AWS docs suggest the other way - that they are triggered by a failed deployment Read more AWS SAM documentation AWS SAM CloudFormation resources specification AWS CodeDeploy documentation AWS Lambda aliases ","permalink":"https://myhightech.org/posts/aws-sam-codedeploy/","summary":"AWS Serverless Application Model I was recently preparing for my AWS DevOps Engineer exam and I wanted to give AWS Serverless Application Model a try. AWS Serverless Application Model is a framework to build and deploy serverless application on AWS using Lambda, API Gateway and DynamoDB.\nUnder the hood it is a AWS CloudFormation transform, which expands the CloudFormation syntax and adds additional resources under the AWS::Serverless namespace. During the template provisioning those resources get expanded to basic CloudFormation resources.","title":"Using AWS SAM and CodeDeploy to deploy serverless applications"}]